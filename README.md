# ml-directory
Machine learning directory

## Papers

Reference papers

 - Batch normalization: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
 - Dropout regularization: [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html)
 - Elastic net regularization: [Regularization and variable selection via the
elastic net](http://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf)


## Books

### Statistics

- [Think Stats: Probability and Statistics for Programmers](http://greenteapress.com/thinkstats/) by Allen Downey - (with free PDF download link)

### Reinforcement Learning:

- [Reinforcement Learning: An Introduction](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html) by Sutton & Barto

## Courses

Machine Learning:

- [Learning from Data](http://work.caltech.edu/telecourse.html)

Deep stuff:

- [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
- [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
- [CS 294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/)

## ML Heuristics

### Quotes

- Keep PCA components to maintain 97-98% of variance (Andrew Ng, First ML MOOC)
- k=10 in k-fold CV (Ambroise McLachlan)
- Use 10x samples the VC-dimension for training (Learning from Data course by Yaser S. Abu-Mostafa)
- Word2vec: Skip Gram works well with small amount of data and is found to represent rare words well. On the other hand, CBOW is faster and has better representations for more frequent words. [Article](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)

### Papers

Papers that account heuristics:

- [A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820)
- [An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf)
- [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf)
- [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)

### Pages

- [CNN Tricks](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)
- [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)
- [Guideline to select the hyperparameters in Deep Learning](https://stats.stackexchange.com/q/95495/57185)
- [ML Fundamentals Checklist](https://microsoft.github.io/code-with-engineering-playbook/machine-learning/ml-fundamentals-checklist/) by Microsoft

## Topics

### AutoML - Automated Machine Learning:
- [AutoML.org](http://automl.org/)
- AutoML Workshops:
  - [2014](https://sites.google.com/site/automlwsicml14/) @ICML
  - [2015](https://sites.google.com/site/automlwsicml15/) @ICML
  - [2016](https://sites.google.com/site/automl2016/) @ICML
- [MetaSel Workshop 2014](http://metasel2014.inescporto.pt/) - Meta-Learning and Algorithm Selection @ECAI

## Web sites

- http://deeplearning.net/
- [Data Science Trello public board](https://trello.com/b/rbpEfMld/data-science)

## Tutorials

- [Deep Learning tutorials directory](http://deeplearning.net/tutorial/)
- [Kaggle tutorials directory](https://www.kaggle.com/wiki/Tutorials)
- [Unsupervised Feature Learning and Deep Learning](http://ufldl.stanford.edu/tutorial/)
- [Geoffrey Hinton's Neural Networks for Machine Learning](https://www.youtube.com/playlist?list=PLnWkMhyDLp1DepxsI1pjLBMKnq6INEqKR)
- [General Sequence Learning using Recurrent Neural Networks](https://clip.mn/video/yt-VINCQghQRuM)
- [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)
- [Deep Learning tutorial @ Kaggle](https://www.kaggle.com/c/second-annual-data-science-bowl/details/deep-learning-tutorial)
- [Andrew W. Moore tutorials](https://www.cs.cmu.edu/~awm/tutorials.html) on several aspects of statistical data mining.

### CNN

- [CNN for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)
- [A Comprehensive Guide to Convolutional Neural Networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)

### RNN

- [RNN tutorial](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/) in 4 parts
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## Software

- https://www.r-project.org/ [R]
- http://www.scipy.org/ [Python]
- [TensorFlow](http://www.tensorflow.org/)
- [pandas](http://pandas.pydata.org/) - Python Data Analysis Library
- [xgboost](https://github.com/dmlc/xgboost)

### Deep Learning: 
- [Theano](http://www.deeplearning.net/software/theano/)
- [DL4J](http://deeplearning4j.org/)
- [Caffe](http://caffe.berkeleyvision.org/)
- [torch](http://torch.ch/)

## Blog roll

- [Andrej Karpathy](http://karpathy.github.io/)
